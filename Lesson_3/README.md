# Задание 1
**1. Анализ влияния количества скрытых слоёв без Dropout и BatchNorm**

- Модели без дополнительных слоёв (Dropout, BatchNorm) склонны к переобучению, особенно при увеличении числа слоёв.
- На CIFAR-10 лучшую производительность показала модель с 3 слоями (2 скрытых).
- Аналогичным образом, на MNIST оптимальной оказалась модель с 3 слоями (2 скрытых).

**2. Влияние добавления слоёв с Dropout и BatchNorm**

- При добавлении этих слоёв модели реже переобучались, однако зачастую наблюдалась стагнация в улучшении точности.
- На CIFAR-10 оптимальной оказалась модель с 5 слоями (4 скрытых).
- Аналогично, на MNIST лучше работала модель с 5 слоями (4 скрытых).

Вот улучшенная и более чёткая версия текста:

---

## Задание 2: Эксперименты с шириной сети

Количество параметров напрямую влияет на вычислительную нагрузку: с увеличением числа параметров время обучения также возрастает.

### Выводы:

* **Постоянная ширина слоёв**
  Наилучшие результаты достигаются при:

  * глубине **3** и базовом количестве нейронов **16**
  * глубине **5** и базовом количестве **64**

* **Расширяющаяся архитектура** (например: 32 → 64 → 128)
  Оптимальные результаты показала модель:

  * глубиной **6** с базовым количеством **32**
  * чуть хуже, но сравнимо — с базовым количеством **64**

* **Сужающаяся архитектура** (например: 128 → 64 → 32)
  Лучшая конфигурация:

  * глубина **4**, базовое количество **64**


## Задание 3. Регуляризация нейросетей

## Результаты экспериментов

### Сравнение техник регуляризации

| Конфигурация         | Точность (%) | Время (с) |
| :------------------- | -----------: | --------: |
| `base`               |        52.28 |      92.9 |
| `batch_norm`         |        56.65 |      90.6 |
| `drop_10`            |        52.29 |      89.5 |
| `drop_30`            |        48.81 |      89.0 |
| `drop_30_batch_norm` |        56.24 |      93.3 |
| `drop_50`            |        41.35 |      89.6 |
| `drop_80`            |        12.78 |      90.1 |
| `base_l2`            |        50.52 |      94.6 |

**Вывод**: Наилучшие результаты были достигнуты при использовании:

* **BatchNorm**
* **Комбинации Dropout(0.3) + BatchNorm**

Использование Dropout с высокой вероятностью (p > 0.5) заметно ухудшает точность.

---

### Адаптивная регуляризация

| Конфигурация     | Точность (%) | Время (с) |
| :--------------- | -----------: | --------: |
| `batchnorm_1e-2` |        53.63 |      90.0 |
| `batchnorm_1e-3` |        55.36 |      90.0 |
| `drop_10_30`     |        50.93 |      89.2 |
| `drop_30_10`     |        50.16 |      88.8 |

**Вывод**:

* Изменение `momentum` в BatchNorm не дало устойчивого прироста точности.
* Dropout с разными коэффициентами на разных слоях не улучшил результаты по сравнению с фиксированным значением.

**Рекомендации**:

* Наиболее эффективной является **схема с BatchNorm**
* **Dropout с небольшими значениями вероятности (p ≤ 0.3)** может дополнительно повысить обобщающую способность модели
